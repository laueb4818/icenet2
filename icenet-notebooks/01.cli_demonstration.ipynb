{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceNet: Basic Command-Line Usage\n",
    "\n",
    "## Context\n",
    "\n",
    "### Purpose\n",
    "The IceNet library provides the ability to download, process, train and predict from end to end. Users can interact with IceNet either via the python interface (see notebook 3: library usage) or via a set of command-line interfaces (CLI) which provide a high-level interface.\n",
    "\n",
    "This notebook illustrates the CLI utilities that are available natively from the library for testing and producing operational forecasts. Via this interface, users can specify data inputs, data processing, training models, using them for predictions and processing outputs.\n",
    "\n",
    "### Modelling approach\n",
    "This modelling approach allows users to immediately utilise the library for producing sea ice concentration forecasts.\n",
    "\n",
    "### Highlights\n",
    "The key stages of an end to end run are: \n",
    "* [1. Setup](#1.-Setup)\n",
    "* [2. Download](#2.-Download)\n",
    "* [3. Process](#3.-Process)\n",
    "* [4. Train](#4.-Train)\n",
    "* [5. Predict](#5.-Predict)\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "\n",
    "James Byrne (author)\n",
    "\n",
    "David Wilby\n",
    "\n",
    "Bryn Noel Ubald\n",
    "\n",
    "__Please raise issues [in this repository](https://github.com/icenet-ai/icenet-notebooks/issues) to suggest updates to this notebook!__ \n",
    "\n",
    "Contact me at _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    "\n",
    "#### Modelling codebase\n",
    "James Byrne (code author), Tom Andersson (science author)\n",
    "\n",
    "#### Modelling publications\n",
    "Andersson, T.R., Hosking, J.S., Pérez-Ortiz, M. et al. Seasonal Arctic sea ice forecasting with probabilistic deep learning. Nat Commun 12, 5124 (2021). https://doi.org/10.1038/s41467-021-25257-4\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and British Antarctic Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "In order to execute the IceNet CLI tools in this notebook you will need:\n",
    "\n",
    "* An internet connection is needed for downloading the source data at the beginning of the notebook,\n",
    "* A suitable place to run this jupyter notebook such as:\n",
    "  * Running `jupyter notebook` or `jupyter lab` on your computer ([see the jupyter project page for more](https://docs.jupyter.org/en/latest/install.html)),\n",
    "  * A jupyterhub instance,\n",
    "  * A development environment such as [visual studio code](https://code.visualstudio.com/) which [can run jupyter notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) (Note: for vscode, you'll need to install `ipykernel` in our conda environment later on), or\n",
    "  * A [Google colab](https://colab.research.google.com/) instance.\n",
    "* A working installation of [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html),\n",
    "* GPUs are required for training (due to size of network, unrealistic to try running on CPU) but not required for predictions.\n",
    "* Knowledge of [Git](https://swcarpentry.github.io/git-novice/), [python](https://swcarpentry.github.io/python-novice-inflammation/) and [shell](https://swcarpentry.github.io/shell-novice/) (links to Carpentries courses on these topics)\n",
    "* There are a few external facilities that we interface with, which you will need to set up if you haven't already.\n",
    "  * Data sources under [Climate and Sea Ice Data](#Climate-and-Sea-Ice-Data) including an account and API token for the [Climate Data Store](https://cds.climate.copernicus.eu/#!/home) (detailed later)\n",
    "  * [Wandb](https://wandb.ai/) (Weights and Biases) - which can __optionally__ be used during training for monitoring.\n",
    "\n",
    "We'll assume that you're running in a local copy of `icenet-notebooks` for this tutorial, and __that one directory up we can deposit other repositories and folders__. If you already have some previous IceNet data available (as we do in `../data`) then you can symlink to it using `ln -s ../data`. The reason for this is described further below, as is the creation of this folder if it doesn't exist.\n",
    "\n",
    "eg\n",
    "\n",
    "```\n",
    "my-icenet-project/\n",
    "├── data/\n",
    "└── icenet-notebooks/   <--- we're in here!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "We recommend running IceNet (or any python code) in a virtual environment. Here we will use `conda` to create a virtual envrionment containing `python` and `icenet`:\n",
    "\n",
    "1. First create a conda environment if you don't have one already:  In a shell (not in this notebook), run `conda create -n icenet python=3.11` which creates an environment named `icenet` and installs python 3.11 within it. Follow the prompts at your terminal to complete creation of the environment.\n",
    "1. Activate your environment: `conda activate icenet`,\n",
    "1. Check that your environment has activated correctly: `which python` should return a path to a python installation corresponding to your new environment (e.g. it should say `icenet` in it somewhere),\n",
    "1. Use `pip` to install icenet from the Python Package Index (PyPI): `pip install icenet` which should install the IceNet package and most of its dependencies. (`conda` can be used later to install some other dependencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands\n",
    "\n",
    "Once the icenet library is installed, you'll be able to access all commands made available by the library. Some are utilities that won't be covered, but using `icenet_<TAB>`-complete you should be able to see a list that includes (but ___is not limited to___):\n",
    "\n",
    "* `icenet_data_cmip`\n",
    "* `icenet_data_era5`\n",
    "* `icenet_data_hres`\n",
    "* `icenet_data_masks`\n",
    "* `icenet_data_sic`\n",
    "* `icenet_dataset_create`\n",
    "* `icenet_output`\n",
    "* `icenet_predict`\n",
    "* `icenet_process_cmip`\n",
    "* `icenet_process_era5`\n",
    "* `icenet_process_hres`\n",
    "* `icenet_process_metadata`\n",
    "* `icenet_process_sic`\n",
    "* `icenet_train`\n",
    "* `icenet_plot_forecast`\n",
    "\n",
    "All of these commands are either directly or indirectly (through pipeline shell scripts) used in this notebook...\n",
    "\n",
    "All commands accept options such as `-v` for turning on verbose logging and `-h` for obtaining help about what options they offer. ___As with many shell commands, use `-h` to obtain information about options___.\n",
    "\n",
    "### CLI vs Library vs Pipeline usage\n",
    "\n",
    "The IceNet package is designed to support automated runs from end to end by exposing the above CLI operations. These are simple wrappers around the library itself, and __any__ step of this can be undertaken manually or programmatically by inspecting the relevant endpoints. \n",
    "\n",
    "IceNet can be run in a number of ways: from the command line, the python interface, or as a pipeline.\n",
    "\n",
    "The rule of thumb to follow: \n",
    "\n",
    "* Use the [pipeline repository](https://github.com/icenet-ai/icenet-pipeline) if you want to run the end to end IceNet processing out of the box.\n",
    "* Adapt or customise this process using `icenet_*` commands described in this notebook and in the scripts contained in the [pipeline repo](https://github.com/icenet-ai/icenet-pipeline).\n",
    "* For ultimate customisation, you can interact with the IceNet repository programmatically (which is how the CLI commands operate.) For more information look at the [IceNet CLI implementations](https://github.com/JimCircadian/icenet2/blob/main/setup.py#L32) and the [library notebook](03.library_usage.ipynb), along with the [library documentation](#TODO). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "## 2. Download\n",
    "\n",
    "Now we can get started, with the first step of downloading the data.\n",
    "\n",
    "### Mask data\n",
    "\n",
    "IceNet relies on some generated masks for training/prediction, which can be automatically produced very easily using `icenet_data_masks {north,south}`, which downloads and processes the data required.\n",
    "Once this has been run once, it does not need to be run again since the mask files are stored on disk (the masks vary each month, but are fixed across any given year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_01.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_02.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_03.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_04.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_05.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_06.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_07.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_08.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_09.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_10.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_11.npy, already exists\n",
      "[12-04-24 12:29:14 :INFO    ] - Skipping ./data/masks/south/masks/active_grid_cell_mask_12.npy, already exists\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_masks south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates the following directories/files.\n",
    "\n",
    "<details>\n",
    "  <summary>Directory structure</summary>\n",
    "\n",
    "```\n",
    "icenet-notebooks/ \n",
    "    └── data/masks/south/\n",
    "        ├──  masks/\n",
    "        │   ├──  active_grid_cell_mask_01.npy <--- Mask for the active regions to consider for each month (This is for Jan)\n",
    "        │   ├──  active_grid_cell_mask_02.npy <--- Mask for Feb\n",
    "        │   ├──  ...\n",
    "        │   ├──  check.py\n",
    "        │   ├──  land_mask.npy  <--- This masks the land regions\n",
    "        │   └──  masks.params   <--- This stores details relating to the \"polar hole\"\n",
    "        └──  siconca/           <--- These are temporarily downloaded data used to generate the above masks\n",
    "            └──  2000/\n",
    "                ├──  01/\n",
    "                │   └──  ice_conc_sh_ease2-250_cdr-v2p0_200001021200.nc\n",
    "                ├──  02/\n",
    "                │   └──  ice_conc_sh_ease2-250_cdr-v2p0_200002021200.nc\n",
    "                └── .../\n",
    "                    └── .../\n",
    "```\n",
    "</details>\n",
    "\n",
    "**Note:** The output data structure in its entirety for all parts of the IceNet library is covered in the third notebook ([03.data_and_forecasts.ipynb](03.data_and_forecasts.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Climate and Sea Ice Data\n",
    "\n",
    "Obtaining and preparing data is simply achieved using `icenet_data_*` commands (you need to __configure the [CDS API](https://cds.climate.copernicus.eu/) token yourself__ - see [here](https://cds.climate.copernicus.eu/api-how-to) for some instructions on registering and on how to use the CDS API), which share common arguments `hemisphere`, `start_date` and `end_date`. There are also implementation specific options worth reviewing under `--help`. We specify the variables and levels via these commands.\n",
    "\n",
    "_Please ignore \"NOT IMPLEMENTED YET\", this is indicative of the commands not checking before overwriting files._\n",
    "\n",
    "__The `-d` flag prevents the downloaded data from being downloaded each time.__\n",
    "\n",
    "___Even small data ranges like this can take a while to retrieve (each variable in this case, for four months, is 3GB, so may take up to an hour.) Please refer to [CDS requests page](https://cds.climate.copernicus.eu/cdsapp#!/yourrequests) to monitor ERA5 downloads...___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`icenet_data_era5` downloads <abbr title=\"ERA5 provides hourly estimates of a large number of atmospheric, land and oceanic climate variables. The data cover the Earth on a 30km grid and resolve the atmosphere using 137 levels from the surface up to a height of 80km. ERA5 includes information about uncertainties for all variables at reduced spatial and temporal resolutions.\">ERA5</abbr> (European Centre for Medium Range Weather Forecasting Reanalysis) data. For more information on the ERA5 data, [see the Copernicus page](https://climate.copernicus.eu/climate-reanalysis).\n",
    "\n",
    "We can use the `--help` flag for the command line tools to print the help text and explanation of the options. Some of these help commands can take up to a minute to run, so don't worry if you have to wait a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: icenet_data_era5 [-h] [-c {cdsapi,toolbox}] [-w WORKERS] [-po] [-d]\n",
      "                        [-v] [--vars VARS] [--levels LEVELS] [-n] [-p]\n",
      "                        {north,south} start_date end_date\n",
      "\n",
      "positional arguments:\n",
      "  {north,south}\n",
      "  start_date\n",
      "  end_date\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -c {cdsapi,toolbox}, --choice {cdsapi,toolbox}\n",
      "  -w WORKERS, --workers WORKERS\n",
      "  -po, --parallel-opens\n",
      "                        Allow xarray mfdataset to work with parallel opens\n",
      "  -d, --dont-delete\n",
      "  -v, --verbose\n",
      "  --vars VARS           Comma separated list of vars\n",
      "  --levels LEVELS       Comma separated list of pressures/depths as needed,\n",
      "                        use zero length string if None (e.g. ',,500,,,') and\n",
      "                        pipes for multiple per var (e.g. ',,250|500,,'\n",
      "  -n, --do-not-download\n",
      "  -p, --do-not-postprocess\n"
     ]
    }
   ],
   "source": [
    "# Please note that on some systems running the help commands can take 30 seconds or more the first time it is run.\n",
    "!icenet_data_era5 --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-vars` flag is used for specifying the variables from ERA5 that we want as follows:\n",
    "\n",
    "* `tas`: `2 metre temperature`,\n",
    "* `uas`: `10 metre U wind component`,\n",
    "* `vas`: `10 metre V wind component`,\n",
    "* `zg`: `Geopotential height`.\n",
    "\n",
    "These are the four we'll use here, though others are available.\n",
    "\n",
    "`--levels` specifies the levels requested, here we use the string `,,,500|250` to request `None` for our first four variables and 500 and 250 for our `zg` variable using the syntax `500|250`.\n",
    "\n",
    "Finally, we pass start and end dates for our query. This is in the format of `yyyy-mm-yy`, though for single digits, you can omit the leading 0. So, the both of these are equivalent and valid: `2020-1-1` or `2020-01-01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12-04-24 12:29:17 :INFO    ] - ERA5 Data Downloading\n",
      "[12-04-24 12:29:17 :WARNING ] - !!! Deletions of temp files are switched off: be careful with this, you need to manage your files manually\n",
      "[12-04-24 12:29:17 :INFO    ] - Building request(s), downloading and daily averaging from ERA5 API\n",
      "[12-04-24 12:29:17 :INFO    ] - Processing single download for uas @ None with 121 dates\n",
      "[12-04-24 12:29:17 :INFO    ] - Processing single download for vas @ None with 121 dates\n",
      "[12-04-24 12:29:17 :INFO    ] - Processing single download for tas @ None with 121 dates\n",
      "[12-04-24 12:29:17 :INFO    ] - Processing single download for zg @ 500 with 121 dates\n",
      "[12-04-24 12:29:17 :INFO    ] - Processing single download for zg @ 250 with 121 dates\n",
      "[12-04-24 12:29:18 :INFO    ] - No requested dates remain, likely already present\n",
      "[12-04-24 12:29:18 :INFO    ] - No requested dates remain, likely already present\n",
      "[12-04-24 12:29:18 :INFO    ] - No requested dates remain, likely already present\n",
      "[12-04-24 12:29:18 :INFO    ] - No requested dates remain, likely already present\n",
      "[12-04-24 12:29:18 :INFO    ] - No requested dates remain, likely already present\n",
      "[12-04-24 12:29:18 :INFO    ] - 0 daily files downloaded\n",
      "[12-04-24 12:29:18 :INFO    ] - No regrid batches to processing, moving on...\n",
      "[12-04-24 12:29:18 :INFO    ] - Rotating wind data prior to merging\n",
      "/data/hpcdata/users/bryald/miniconda3/envs/icenet0.2.8/lib/python3.11/site-packages/iris/__init__.py:354: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  cubes = _load_collection(uris, constraints, callback).cubes()\n",
      "[12-04-24 12:29:18 :INFO    ] - Rotating wind data in ./data/era5/south/uas ./data/era5/south/vas\n",
      "[12-04-24 12:29:18 :INFO    ] - 0 files for uas\n",
      "[12-04-24 12:29:18 :INFO    ] - 0 files for vas\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_era5 south -d --vars uas,vas,tas,zg --levels ',,,500|250' 2020-1-1 2020-4-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`icenet_data_sic` downloads the Sea Ice Concentration (SIC) data from the Ocean and Sea Ice Satellite Application Facility (OSI SAF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: icenet_data_sic [-h] [-w WORKERS] [-po] [-d] [-v] [-u]\n",
      "                       [-c SIC_CHUNKING_SIZE] [-dt DASK_TIMEOUTS]\n",
      "                       [-dp DASK_PORT]\n",
      "                       {north,south} start_date end_date\n",
      "\n",
      "positional arguments:\n",
      "  {north,south}\n",
      "  start_date\n",
      "  end_date\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -w WORKERS, --workers WORKERS\n",
      "  -po, --parallel-opens\n",
      "                        Allow xarray mfdataset to work with parallel opens\n",
      "  -d, --dont-delete\n",
      "  -v, --verbose\n",
      "  -u, --use-dask\n",
      "  -c SIC_CHUNKING_SIZE, --sic-chunking-size SIC_CHUNKING_SIZE\n",
      "  -dt DASK_TIMEOUTS, --dask-timeouts DASK_TIMEOUTS\n",
      "  -dp DASK_PORT, --dask-port DASK_PORT\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_sic --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run `icenet_data_sic` you will need the `eccodes` package installed. If you have used `pip` to install IceNet, you will need to use `conda` to install `eccodes` by running `conda install -c conda-forge eccodes` at the command line. Alternatively, the ECMWF provide alternative instructions for installing eccodes [here](https://confluence.ecmwf.int/display/ECC/ecCodes+installation#ecCodesinstallation-Python3bindings).\n",
    "\n",
    "Here we pass two dates bounding the data range to be downloaded using the `-d` option with the dates in `YYYY-M-D` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12-04-24 12:29:20 :INFO    ] - OSASIF-SIC Data Downloading\n",
      "[12-04-24 12:29:20 :INFO    ] - Downloading SIC datafiles to .temp intermediates...\n",
      "[12-04-24 12:29:21 :INFO    ] - Excluding 366 dates already existing from 121 dates requested.\n",
      "[12-04-24 12:29:21 :INFO    ] - Opening for interpolation: ['./data/osisaf/south/siconca/2020.nc']\n",
      "[12-04-24 12:29:21 :INFO    ] - Processing 0 missing dates\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_sic south -d 2020-1-1 2020-4-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "By default, the IceNet commands regrid and rotates data as required to align with the OSISAF SIC data, which is used as the output for the dataset. Programmatic usage allows you to avoid this (see [03.library_usage](03.library_usage.ipynb)).\n",
    "\n",
    "The following downloaders are available:\n",
    "\n",
    "* `icenet_data_era5` - downloads [ERA5 reanalysis](https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset&keywords=((%20%22Product%20type:%20Reanalysis%22%20))) data using either the CDS Toolbox or direct API\n",
    "* `icenet_data_cmip` - downloads the prescribed experiments from [CMIP6](https://esgf-node.llnl.gov/search/cmip6/) for the original IceNet paper runs\n",
    "* `icenet_data_hres` - downloads up to date [forecast generated data from the ECMWF MARS API](https://www.ecmwf.int/en/forecasts/datasets/catalogue-ecmwf-real-time-products)\n",
    "* `icenet_data_sic` - downloads [OSISAF sea-ice concentration (SIC) data](https://osisaf-hl.met.no/v2p1-sea-ice-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "## 3. Process\n",
    "\n",
    "Processing takes the data made available through the source data store and undertakes the necessary normalisation for use as input channels to the UNet architecture. This intermediary step means that the original source data can be reused numerous times with varying training, validation and test date setups.\n",
    "\n",
    "### Command example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: icenet_process_era5 [-h] [-ns TRAIN_START] [-ne TRAIN_END]\n",
      "                           [-vs VAL_START] [-ve VAL_END] [-ts TEST_START]\n",
      "                           [-te TEST_END] [-l LAG] [-f FORECAST] [-po]\n",
      "                           [--abs ABS] [--anom ANOM] [--trends TRENDS]\n",
      "                           [--trend-lead TREND_LEAD] [-r REF] [-v]\n",
      "                           [-u UPDATE_KEY]\n",
      "                           name {north,south}\n",
      "\n",
      "positional arguments:\n",
      "  name\n",
      "  {north,south}\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -ns TRAIN_START, --train_start TRAIN_START\n",
      "  -ne TRAIN_END, --train_end TRAIN_END\n",
      "  -vs VAL_START, --val_start VAL_START\n",
      "  -ve VAL_END, --val_end VAL_END\n",
      "  -ts TEST_START, --test-start TEST_START\n",
      "  -te TEST_END, --test-end TEST_END\n",
      "  -l LAG, --lag LAG\n",
      "  -f FORECAST, --forecast FORECAST\n",
      "  -po, --parallel-opens\n",
      "                        Allow xarray mfdataset to work with parallel opens\n",
      "  --abs ABS             Comma separated list of abs vars\n",
      "  --anom ANOM           Comma separated list of abs vars\n",
      "  --trends TRENDS       Comma separated list of abs vars\n",
      "  --trend-lead TREND_LEAD\n",
      "                        Time steps in the future for linear trends\n",
      "  -r REF, --ref REF     Reference loader for normalisations etc\n",
      "  -v, --verbose\n",
      "  -u UPDATE_KEY, --update-key UPDATE_KEY\n",
      "                        Add update key to processor to avoid overwriting\n",
      "                        defaultentries in the loader configuration\n"
     ]
    }
   ],
   "source": [
    "!icenet_process_era5 --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands take the following as positional arguments:\n",
    "\n",
    "| argument    | description                                   | value         |\n",
    "|     ---:    |:---                                           | :---          |\n",
    "|*NAME*       | Processed data output name                    | tutorial_data |\n",
    "|*HEMISPHERE* | Hemisphere(s) the processed data covers       | south         |\n",
    "\n",
    "This outputs the processed files into the `processed/tutorial_data` directory and creates a loader file called `loader.tutorial_data.json`. Each of these `process` commands updates the loader file with the corresponding information on how the processed data was generated as a form of data lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12-04-24 12:29:27 :INFO    ] - Got 91 dates for train\n",
      "[12-04-24 12:29:27 :INFO    ] - Got 21 dates for val\n",
      "[12-04-24 12:29:27 :INFO    ] - Got 2 dates for test\n",
      "[12-04-24 12:29:27 :INFO    ] - Creating path: ./processed/tutorial_data/era5\n",
      "[12-04-24 12:29:27 :INFO    ] - Processing 91 dates for train category\n",
      "[12-04-24 12:29:27 :INFO    ] - Including lag of 1 days\n",
      "[12-04-24 12:29:27 :INFO    ] - Including lead of 93 days\n",
      "[12-04-24 12:29:33 :INFO    ] - Processing 21 dates for val category\n",
      "[12-04-24 12:29:33 :INFO    ] - Including lag of 1 days\n",
      "[12-04-24 12:29:33 :INFO    ] - Including lead of 93 days\n",
      "[12-04-24 12:29:33 :INFO    ] - Processing 2 dates for test category\n",
      "[12-04-24 12:29:33 :INFO    ] - Including lag of 1 days\n",
      "[12-04-24 12:29:33 :INFO    ] - Including lead of 93 days\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for hus1000\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for psl\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for rlds\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for rsds\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for ta500\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for tas\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for tos\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for uas\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for vas\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for zg250\n",
      "[12-04-24 12:29:33 :INFO    ] - Got 2 files for zg500\n",
      "[12-04-24 12:29:33 :INFO    ] - Opening files for uas\n",
      "[12-04-24 12:29:33 :INFO    ] - Filtered to 731 units long based on configuration requirements\n",
      "[12-04-24 12:29:35 :INFO    ] - Normalising uas\n",
      "[12-04-24 12:29:36 :INFO    ] - Opening files for vas\n",
      "[12-04-24 12:29:36 :INFO    ] - Filtered to 731 units long based on configuration requirements\n",
      "[12-04-24 12:29:38 :INFO    ] - Normalising vas\n",
      "[12-04-24 12:29:39 :INFO    ] - Opening files for tas\n",
      "[12-04-24 12:29:39 :INFO    ] - Filtered to 731 units long based on configuration requirements\n",
      "[12-04-24 12:29:39 :INFO    ] - Generating climatology ./processed/tutorial_data/era5/south/params/climatology.tas\n",
      "[12-04-24 12:29:39 :WARNING ] - We don't have a full climatology (1,2,3) compared with data (1,2,3,4,5,6,7,8,9,10,11,12)\n",
      "[12-04-24 12:29:40 :INFO    ] - Normalising tas\n",
      "[12-04-24 12:29:42 :INFO    ] - Opening files for zg500\n",
      "[12-04-24 12:29:42 :INFO    ] - Filtered to 731 units long based on configuration requirements\n",
      "[12-04-24 12:29:42 :INFO    ] - Generating climatology ./processed/tutorial_data/era5/south/params/climatology.zg500\n",
      "[12-04-24 12:29:42 :WARNING ] - We don't have a full climatology (1,2,3) compared with data (1,2,3,4,5,6,7,8,9,10,11,12)\n",
      "[12-04-24 12:29:43 :INFO    ] - Normalising zg500\n",
      "[12-04-24 12:29:44 :INFO    ] - Opening files for zg250\n",
      "[12-04-24 12:29:44 :INFO    ] - Filtered to 731 units long based on configuration requirements\n",
      "[12-04-24 12:29:44 :INFO    ] - Generating climatology ./processed/tutorial_data/era5/south/params/climatology.zg250\n",
      "[12-04-24 12:29:44 :WARNING ] - We don't have a full climatology (1,2,3) compared with data (1,2,3,4,5,6,7,8,9,10,11,12)\n",
      "[12-04-24 12:29:45 :INFO    ] - Normalising zg250\n",
      "[12-04-24 12:29:46 :INFO    ] - Writing configuration to ./loader.tutorial_data.json\n",
      "[12-04-24 12:29:49 :INFO    ] - Got 91 dates for train\n",
      "[12-04-24 12:29:49 :INFO    ] - Got 20 dates for val\n",
      "[12-04-24 12:29:49 :INFO    ] - Got 2 dates for test\n",
      "[12-04-24 12:29:49 :INFO    ] - Creating path: ./processed/tutorial_data/osisaf\n",
      "[12-04-24 12:29:49 :INFO    ] - Processing 91 dates for train category\n",
      "[12-04-24 12:29:49 :INFO    ] - Including lag of 1 days\n",
      "[12-04-24 12:29:49 :INFO    ] - Including lead of 93 days\n",
      "[12-04-24 12:29:51 :INFO    ] - Processing 20 dates for val category\n",
      "[12-04-24 12:29:51 :INFO    ] - Including lag of 1 days\n",
      "[12-04-24 12:29:51 :INFO    ] - Including lead of 93 days\n",
      "[12-04-24 12:29:51 :INFO    ] - Processing 2 dates for test category\n",
      "[12-04-24 12:29:51 :INFO    ] - Including lag of 1 days\n",
      "[12-04-24 12:29:51 :INFO    ] - Including lead of 93 days\n",
      "[12-04-24 12:29:51 :INFO    ] - Got 2 files for siconca\n",
      "[12-04-24 12:29:51 :INFO    ] - Opening files for siconca\n",
      "[12-04-24 12:29:51 :INFO    ] - Filtered to 731 units long based on configuration requirements\n",
      "[12-04-24 12:29:54 :INFO    ] - No normalisation for siconca\n",
      "[12-04-24 12:29:55 :INFO    ] - Loading configuration ./loader.tutorial_data.json\n",
      "[12-04-24 12:29:55 :INFO    ] - Writing configuration to ./loader.tutorial_data.json\n",
      "[12-04-24 12:29:58 :INFO    ] - Creating path: ./processed/tutorial_data/meta\n",
      "[12-04-24 12:29:58 :INFO    ] - Loading configuration ./loader.tutorial_data.json\n",
      "[12-04-24 12:29:58 :INFO    ] - Writing configuration to ./loader.tutorial_data.json\n"
     ]
    }
   ],
   "source": [
    "!icenet_process_era5 tutorial_data south \\\n",
    "    -ns 2020-1-1 -ne 2020-3-31 -vs 2020-4-3 -ve 2020-4-23 -ts 2020-4-1 -te 2020-4-2 \\\n",
    "    -l 1 --abs uas,vas --anom tas,zg500,zg250\n",
    "\n",
    "!icenet_process_sic tutorial_data south \\\n",
    "    -ns 2020-1-1 -ne 2020-3-31 -vs 2020-4-1 -ve 2020-4-20 -ts 2020-4-1 -te 2020-4-2 \\\n",
    "    -l 1 --abs siconca\n",
    "\n",
    "!icenet_process_metadata tutorial_data south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Consulting the command options will make the above more obvious (as well as further options) but a few things we can note that are helpful: \n",
    "\n",
    "* Options `-ns`, `-ne`, `-vs`, `-ve`, `-ts`, `-te`, which correspond to training, validation and test sets, allow ranges to be comma-delimited. The above example produces a split training set, for example, that spans the first 4 months of 2020.\n",
    "  * `-ns` specifies the *start* of the training set, `-ne` specifies the *end*.\n",
    "  * `-vs` specifies the *start* of the validation set, `-ve` specifies the *end*.\n",
    "  * `-ts` specifies the *start* of the test set, `-te` specifies the *end*.\n",
    "* These date ranges can be randomised and subsampled using `-d`, __though this is still a bit experimental__\n",
    "* The `-l` option (which is for `--lag`) specified the number of days back we look at input data variables for the output in question.\n",
    "\n",
    "There are plenty of other options available for preprocessing the data, but it should be noted that whilst this is not strongly coupled to dataset creation, options like the lag specified here might influence the creation of datasets in the next step. \n",
    "\n",
    "These commands, especially with decadal ranges, can take a long time (12+ hours) to complete depending on the hosts/storage in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "\n",
    "Once the above preprocessing is taken care of datasets can easily be created thus. This operation _creates a cached dataset_ in the filesystem that can be fed in for training runs. \n",
    "\n",
    "The common options used here: \n",
    "\n",
    "* `-fd` allows us to specify how far forward to forecast to. For this example we're limiting to 7 days based on the limited amount of SIC groud truth data we downloaded.\n",
    "* `-l` as in the preprocessing stage. If experimenting and using full date ranges, creating a dataset with a different lag can save having to reprocess everything.\n",
    "* `-ob` is the output batch size for the tfrecords. It is advisable to keep this smaller except where there are seriously large numbers of sets, preferably near to the expected size being used for training.\n",
    "* `-w` specifies the number of worker subprocesses to use for producing the output. Probably advisable to keep this below the number of cores on your host! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12-04-24 12:30:01 :INFO    ] - Got 0 dates for train\n",
      "[12-04-24 12:30:01 :INFO    ] - Got 0 dates for val\n",
      "[12-04-24 12:30:01 :INFO    ] - Got 0 dates for test\n",
      "[12-04-24 12:30:01 :INFO    ] - Creating path: ./network_datasets/tutorial_data\n",
      "[12-04-24 12:30:01 :INFO    ] - Loading configuration loader.tutorial_data.json\n",
      "/data/hpcdata/users/bryald/miniconda3/envs/icenet0.2.8/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8888 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40412 instead\n",
      "  warnings.warn(\n",
      "[12-04-24 12:30:04 :INFO    ] - Dashboard at localhost:8888\n",
      "[12-04-24 12:30:04 :INFO    ] - Using dask client <Client: 'tcp://127.0.0.1:40471' processes=4 threads=4, memory=503.20 GiB>\n",
      "[12-04-24 12:30:04 :INFO    ] - 91 train dates to process, generating cache data.\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000000.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000001.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000002.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000003.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000004.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000005.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000006.tfrecord\n",
      "[12-04-24 12:30:18 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000007.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000008.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000009.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000010.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000011.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000012.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000013.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000014.tfrecord\n",
      "[12-04-24 12:30:34 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000015.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000016.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000017.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000018.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000019.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000020.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000021.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000022.tfrecord\n",
      "[12-04-24 12:30:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000023.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000024.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000025.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000026.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000027.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000028.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000029.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000030.tfrecord\n",
      "[12-04-24 12:31:03 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000031.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000032.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000033.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000034.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000035.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000036.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000037.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000038.tfrecord\n",
      "[12-04-24 12:31:16 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000039.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000040.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000041.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000042.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000043.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000044.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/train/00000045.tfrecord\n",
      "[12-04-24 12:31:27 :INFO    ] - 23 val dates to process, generating cache data.\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000000.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000001.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000002.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000003.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000004.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000005.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000006.tfrecord\n",
      "[12-04-24 12:31:41 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000007.tfrecord\n",
      "[12-04-24 12:31:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000008.tfrecord\n",
      "[12-04-24 12:31:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000009.tfrecord\n",
      "[12-04-24 12:31:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000010.tfrecord\n",
      "[12-04-24 12:31:48 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/val/00000011.tfrecord\n",
      "[12-04-24 12:31:48 :INFO    ] - 2 test dates to process, generating cache data.\n",
      "[12-04-24 12:31:52 :INFO    ] - Finished output ./network_datasets/tutorial_data/south/test/00000000.tfrecord\n",
      "[12-04-24 12:31:52 :INFO    ] - Average sample generation time: 4.78705077952352\n",
      "[12-04-24 12:31:52 :INFO    ] - Writing configuration to ./dataset_config.tutorial_data.json\n"
     ]
    }
   ],
   "source": [
    "!icenet_dataset_create tutorial_data south -l 1 -fd 7 -ob 2 -w 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Config-only operation / Prediction datasets\n",
    "\n",
    "Datasets used to predict don't benefit from caching, so adding the `-c` option and dropping `-w` and `-ob` will create a configuration for the dataset without writing sets to disk. You can also use this option to create a dataset that is fed directly from the preprocessed data, though bear in mind, depending on your infrastructure, that this requires the batches to be created on the fly and can have a significant impact on performance. By specifying `-fn` we ensure the dataset is given a different name to the previously cached one above (though this is more commonly used for prediction datasets where caching isn't necessary...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12-04-24 12:31:57 :INFO    ] - Got 0 dates for train\n",
      "[12-04-24 12:31:57 :INFO    ] - Got 0 dates for val\n",
      "[12-04-24 12:31:57 :INFO    ] - Got 0 dates for test\n",
      "[12-04-24 12:31:57 :INFO    ] - Creating path: ./network_datasets/tutorial_raw_dataset\n",
      "[12-04-24 12:31:57 :INFO    ] - Loading configuration loader.tutorial_data.json\n",
      "[12-04-24 12:31:57 :INFO    ] - Writing dataset configuration without data generation\n",
      "[12-04-24 12:31:57 :INFO    ] - 91 train dates in total, NOT generating cache data.\n",
      "[12-04-24 12:31:57 :INFO    ] - 23 val dates in total, NOT generating cache data.\n",
      "[12-04-24 12:31:57 :INFO    ] - 2 test dates in total, NOT generating cache data.\n",
      "[12-04-24 12:31:57 :INFO    ] - Writing configuration to ./dataset_config.tutorial_raw_dataset.json\n"
     ]
    }
   ],
   "source": [
    "!icenet_dataset_create -fd 7 -l 1 -c -fn tutorial_raw_dataset tutorial_data south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "## 4. Train\n",
    "\n",
    "Once the dataset is prepared, running a network is then as simple as using `icenet_train` with the appropriate parameters. Some key parameters are illustrated in the following commands:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "usage: icenet_train [-h] [-b BATCH_SIZE] [-ca CHECKPOINT_MODE]\n",
      "                    [-cm CHECKPOINT_MONITOR] [-ds [ADDITIONAL ...]]\n",
      "                    [-e EPOCHS] [-f FILTER_SIZE]\n",
      "                    [--early-stopping EARLY_STOPPING] [-m]\n",
      "                    [-n N_FILTERS_FACTOR] [-p PRELOAD] [-pw]\n",
      "                    [-qs MAX_QUEUE_SIZE] [-r RATIO]\n",
      "                    [-s {default,mirrored,central}] [--shuffle-train]\n",
      "                    [--gpus GPUS] [-v] [-w WORKERS] [-nw] [-wo]\n",
      "                    [-wp WANDB_PROJECT] [-wu WANDB_USER] [--lr LR]\n",
      "                    [--lr_10e_decay_fac LR_10E_DECAY_FAC]\n",
      "                    [--lr_decay_start LR_DECAY_START]\n",
      "                    [--lr_decay_end LR_DECAY_END]\n",
      "                    dataset run_name seed\n",
      "\n",
      "positional arguments:\n",
      "  dataset\n",
      "  run_name\n",
      "  seed\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -b BATCH_SIZE, --batch-size BATCH_SIZE\n",
      "  -ca CHECKPOINT_MODE, --checkpoint-mode CHECKPOINT_MODE\n",
      "  -cm CHECKPOINT_MONITOR, --checkpoint-monitor CHECKPOINT_MONITOR\n",
      "  -ds [ADDITIONAL ...], --additional-dataset [ADDITIONAL ...]\n",
      "  -e EPOCHS, --epochs EPOCHS\n",
      "  -f FILTER_SIZE, --filter-size FILTER_SIZE\n",
      "  --early-stopping EARLY_STOPPING\n",
      "  -m, --multiprocessing\n",
      "  -n N_FILTERS_FACTOR, --n-filters-factor N_FILTERS_FACTOR\n",
      "  -p PRELOAD, --preload PRELOAD\n",
      "  -pw, --pickup-weights\n",
      "  -qs MAX_QUEUE_SIZE, --max-queue-size MAX_QUEUE_SIZE\n",
      "  -r RATIO, --ratio RATIO\n",
      "  -s {default,mirrored,central}, --strategy {default,mirrored,central}\n",
      "  --shuffle-train       Shuffle the training set\n",
      "  --gpus GPUS\n",
      "  -v, --verbose\n",
      "  -w WORKERS, --workers WORKERS\n",
      "  -nw, --no-wandb\n",
      "  -wo, --wandb-offline\n",
      "  -wp WANDB_PROJECT, --wandb-project WANDB_PROJECT\n",
      "  -wu WANDB_USER, --wandb-user WANDB_USER\n",
      "  --lr LR\n",
      "  --lr_10e_decay_fac LR_10E_DECAY_FAC\n",
      "                        Factor by which LR is multiplied by every 10 epochs\n",
      "                        using exponential decay. E.g. 1 -> no decay (default),\n",
      "                        0.5 -> halve every 10 epochs.\n",
      "  --lr_decay_start LR_DECAY_START\n",
      "  --lr_decay_end LR_DECAY_END\n"
     ]
    }
   ],
   "source": [
    "!icenet_train --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following runs demonstrate using the aforementioned dataset with the following options:\n",
    "\n",
    "* in `-b` batches of 2 (`-b 2`)\n",
    "* for a run of `-e` 10 epochs (`-e 10`)\n",
    "* using `-m` for multiprocessing we enable up to `-w` 4 process workers to load data at a time (`-w 4`)\n",
    "* into a data queue `-qs` of length 4 (`-qs 4`)\n",
    "* We could specify a `-r` ratio we use only 0.2x of the files from the dataset (_useful when testing on a low power machine with a large dataset, but unnecessary with our example here_) \n",
    "* supplying a UNet built with 0.6x the `-n` numbers of filters as normal. (`-n 0.6`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "[12-04-24 12:32:04 :WARNING ] - Setting seed for best attempt at determinism, value 42\n",
      "[12-04-24 12:32:04 :INFO    ] - Loading configuration dataset_config.tutorial_data.json\n",
      "[12-04-24 12:32:04 :INFO    ] - Training dataset path: ./network_datasets/tutorial_data/south/train\n",
      "[12-04-24 12:32:04 :INFO    ] - Validation dataset path: ./network_datasets/tutorial_data/south/val\n",
      "[12-04-24 12:32:04 :INFO    ] - Test dataset path: ./network_datasets/tutorial_data/south/test\n",
      "[12-04-24 12:32:04 :WARNING ] - WandB is not available, we will never use it\n",
      "[12-04-24 12:32:04 :INFO    ] - Creating network folder: ./results/networks/tutorial_testrun\n",
      "[12-04-24 12:32:04 :INFO    ] - Adding tensorboard callback\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 432, 432, 9)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 432, 432, 38)         3116      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 432, 432, 38)         13034     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 432, 432, 38)         152       ['conv2d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 216, 216, 38)         0         ['batch_normalization[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 216, 216, 76)         26068     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 216, 216, 76)         52060     ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 216, 216, 76)         304       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 108, 108, 76)         0         ['batch_normalization_1[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 108, 108, 152)        104120    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 108, 108, 152)        208088    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 108, 108, 152)        608       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 54, 54, 152)          0         ['batch_normalization_2[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 54, 54, 152)          208088    ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 54, 54, 152)          208088    ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 54, 54, 152)          608       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 27, 27, 152)          0         ['batch_normalization_3[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 27, 27, 304)          416176    ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 27, 27, 304)          832048    ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 27, 27, 304)          1216      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 54, 54, 304)          0         ['batch_normalization_4[0][0]'\n",
      " D)                                                                 ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 54, 54, 152)          184984    ['up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 54, 54, 304)          0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'conv2d_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 54, 54, 152)          416024    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 54, 54, 152)          208088    ['conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 54, 54, 152)          608       ['conv2d_12[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, 108, 108, 152)        0         ['batch_normalization_5[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 108, 108, 152)        92568     ['up_sampling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 108, 108, 304)        0         ['batch_normalization_2[0][0]'\n",
      " )                                                                  , 'conv2d_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 108, 108, 152)        416024    ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 108, 108, 152)        208088    ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 108, 108, 152)        608       ['conv2d_15[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSamplin  (None, 216, 216, 152)        0         ['batch_normalization_6[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 216, 216, 76)         46284     ['up_sampling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 216, 216, 152)        0         ['batch_normalization_1[0][0]'\n",
      " )                                                                  , 'conv2d_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 216, 216, 76)         104044    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 216, 216, 76)         52060     ['conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 216, 216, 76)         304       ['conv2d_18[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSamplin  (None, 432, 432, 76)         0         ['batch_normalization_7[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 432, 432, 38)         11590     ['up_sampling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 432, 432, 76)         0         ['conv2d_1[0][0]',            \n",
      " )                                                                   'conv2d_19[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 432, 432, 38)         26030     ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 432, 432, 38)         13034     ['conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 432, 432, 38)         13034     ['conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 432, 432, 7)          273       ['conv2d_22[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3867419 (14.75 MB)\n",
      "Trainable params: 3865215 (14.74 MB)\n",
      "Non-trainable params: 2204 (8.61 KB)\n",
      "__________________________________________________________________________________________________\n",
      "[12-04-24 12:32:04 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:32:04 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[12-04-24 12:32:04 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:32:05 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_rmse improved from inf to 41.76014, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "/data/hpcdata/users/bryald/miniconda3/envs/icenet0.2.8/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "46/46 - 42s - loss: 185.0622 - binacc: 51.6062 - mae: 25.1732 - rmse: 31.9053 - mse: 2170.9961 - val_loss: 317.0404 - val_binacc: 36.9813 - val_mae: 39.7584 - val_rmse: 41.7601 - val_mse: 2186.0569 - lr: 1.0000e-04 - 42s/epoch - 908ms/step\n",
      "[12-04-24 12:32:46 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_rmse did not improve from 41.76014\n",
      "46/46 - 36s - loss: 21.3577 - binacc: 94.2935 - mae: 5.7285 - rmse: 10.8388 - mse: 1541.0861 - val_loss: 341.5743 - val_binacc: 36.9843 - val_mae: 41.4914 - val_rmse: 43.3458 - val_mse: 2106.0293 - lr: 1.0000e-04 - 36s/epoch - 787ms/step\n",
      "[12-04-24 12:33:23 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_rmse improved from 41.76014 to 39.31659, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 39s - loss: 13.3029 - binacc: 95.9355 - mae: 4.1533 - rmse: 8.5542 - mse: 1438.9353 - val_loss: 281.0234 - val_binacc: 44.7997 - val_mae: 35.4067 - val_rmse: 39.3166 - val_mse: 1763.3151 - lr: 1.0000e-04 - 39s/epoch - 842ms/step\n",
      "[12-04-24 12:34:01 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_rmse improved from 39.31659 to 33.31392, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 37s - loss: 12.4394 - binacc: 96.1835 - mae: 3.8191 - rmse: 8.2719 - mse: 1422.5914 - val_loss: 201.7634 - val_binacc: 87.5353 - val_mae: 21.5612 - val_rmse: 33.3139 - val_mse: 968.5349 - lr: 1.0000e-04 - 37s/epoch - 810ms/step\n",
      "[12-04-24 12:34:39 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_rmse improved from 33.31392 to 28.72494, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 37s - loss: 9.8561 - binacc: 96.7271 - mae: 3.2957 - rmse: 7.3630 - mse: 1333.9780 - val_loss: 150.0061 - val_binacc: 91.7101 - val_mae: 17.5978 - val_rmse: 28.7249 - val_mse: 650.2924 - lr: 1.0000e-04 - 37s/epoch - 798ms/step\n",
      "[12-04-24 12:35:15 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_rmse improved from 28.72494 to 25.64511, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 37s - loss: 8.5259 - binacc: 97.0250 - mae: 3.0218 - rmse: 6.8482 - mse: 1331.1373 - val_loss: 119.5639 - val_binacc: 93.3521 - val_mae: 15.5600 - val_rmse: 25.6451 - val_mse: 422.7718 - lr: 1.0000e-04 - 37s/epoch - 808ms/step\n",
      "[12-04-24 12:35:53 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_rmse improved from 25.64511 to 23.24992, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 38s - loss: 7.7727 - binacc: 97.2187 - mae: 2.8549 - rmse: 6.5387 - mse: 1342.6183 - val_loss: 98.2729 - val_binacc: 94.3831 - val_mae: 14.1081 - val_rmse: 23.2499 - val_mse: 305.5913 - lr: 1.0000e-04 - 38s/epoch - 819ms/step\n",
      "[12-04-24 12:36:30 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_rmse improved from 23.24992 to 21.65624, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 36s - loss: 7.4537 - binacc: 97.3086 - mae: 2.7836 - rmse: 6.4031 - mse: 1334.3538 - val_loss: 85.2623 - val_binacc: 94.8258 - val_mae: 13.1444 - val_rmse: 21.6562 - val_mse: 272.3158 - lr: 1.0000e-04 - 36s/epoch - 785ms/step\n",
      "[12-04-24 12:37:06 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_rmse improved from 21.65624 to 19.58839, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 37s - loss: 7.2369 - binacc: 97.3701 - mae: 2.7411 - rmse: 6.3093 - mse: 1303.2742 - val_loss: 69.7571 - val_binacc: 95.2500 - val_mae: 11.9701 - val_rmse: 19.5884 - val_mse: 268.8853 - lr: 1.0000e-04 - 37s/epoch - 794ms/step\n",
      "[12-04-24 12:37:43 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_rmse improved from 19.58839 to 15.22561, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 39s - loss: 6.8309 - binacc: 97.4298 - mae: 2.6494 - rmse: 6.1298 - mse: 1235.8749 - val_loss: 42.1445 - val_binacc: 96.0140 - val_mae: 9.7614 - val_rmse: 15.2256 - val_mse: 314.2539 - lr: 1.0000e-04 - 39s/epoch - 839ms/step\n",
      "[12-04-24 12:38:22 :INFO    ] - Saving network to: ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "[12-04-24 12:38:27 :INFO    ] - Running evaluation against test set\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:38:27 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:38:27 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:38:27 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:38:27 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:38:28 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:38:28 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[12-04-24 12:38:28 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:38:28 :INFO    ] - Using test set for validation\n",
      "[12-04-24 12:38:28 :INFO    ] - Metric creation for lead time of 7 days\n",
      "[12-04-24 12:38:28 :INFO    ] - Evaluating... \n",
      "[12-04-24 12:38:31 :INFO    ] - Done in 2.4s\n"
     ]
    }
   ],
   "source": [
    "!icenet_train tutorial_data tutorial_testrun 42 -b 2 -e 10 -m -qs 4 -w 4 -n 0.6 -nw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second command, we can pick up the training from the previous run above and continue training for another 2 epochs, as set by the `-e 2` option.\n",
    "\n",
    "This can be useful when wanting to continue training runs after having run them previously (if say for example you realised later down the line that you did not run for long enough, or due to resource constraints or any other reason, you had to stop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "[12-04-24 12:38:41 :WARNING ] - Setting seed for best attempt at determinism, value 42\n",
      "[12-04-24 12:38:41 :INFO    ] - Loading configuration dataset_config.tutorial_data.json\n",
      "[12-04-24 12:38:41 :INFO    ] - Training dataset path: ./network_datasets/tutorial_data/south/train\n",
      "[12-04-24 12:38:41 :INFO    ] - Validation dataset path: ./network_datasets/tutorial_data/south/val\n",
      "[12-04-24 12:38:41 :INFO    ] - Test dataset path: ./network_datasets/tutorial_data/south/test\n",
      "[12-04-24 12:38:41 :WARNING ] - WandB is not available, we will never use it\n",
      "[12-04-24 12:38:41 :INFO    ] - Adding tensorboard callback\n",
      "[12-04-24 12:38:42 :INFO    ] - Loading network weights from ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 432, 432, 9)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 432, 432, 38)         3116      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 432, 432, 38)         13034     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 432, 432, 38)         152       ['conv2d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 216, 216, 38)         0         ['batch_normalization[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 216, 216, 76)         26068     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 216, 216, 76)         52060     ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 216, 216, 76)         304       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 108, 108, 76)         0         ['batch_normalization_1[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 108, 108, 152)        104120    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 108, 108, 152)        208088    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 108, 108, 152)        608       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 54, 54, 152)          0         ['batch_normalization_2[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 54, 54, 152)          208088    ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 54, 54, 152)          208088    ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 54, 54, 152)          608       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 27, 27, 152)          0         ['batch_normalization_3[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 27, 27, 304)          416176    ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 27, 27, 304)          832048    ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 27, 27, 304)          1216      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 54, 54, 304)          0         ['batch_normalization_4[0][0]'\n",
      " D)                                                                 ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 54, 54, 152)          184984    ['up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 54, 54, 304)          0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'conv2d_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 54, 54, 152)          416024    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 54, 54, 152)          208088    ['conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 54, 54, 152)          608       ['conv2d_12[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, 108, 108, 152)        0         ['batch_normalization_5[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 108, 108, 152)        92568     ['up_sampling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 108, 108, 304)        0         ['batch_normalization_2[0][0]'\n",
      " )                                                                  , 'conv2d_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 108, 108, 152)        416024    ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 108, 108, 152)        208088    ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 108, 108, 152)        608       ['conv2d_15[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSamplin  (None, 216, 216, 152)        0         ['batch_normalization_6[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 216, 216, 76)         46284     ['up_sampling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 216, 216, 152)        0         ['batch_normalization_1[0][0]'\n",
      " )                                                                  , 'conv2d_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 216, 216, 76)         104044    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 216, 216, 76)         52060     ['conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 216, 216, 76)         304       ['conv2d_18[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSamplin  (None, 432, 432, 76)         0         ['batch_normalization_7[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 432, 432, 38)         11590     ['up_sampling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 432, 432, 76)         0         ['conv2d_1[0][0]',            \n",
      " )                                                                   'conv2d_19[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 432, 432, 38)         26030     ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 432, 432, 38)         13034     ['conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 432, 432, 38)         13034     ['conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 432, 432, 7)          273       ['conv2d_22[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3867419 (14.75 MB)\n",
      "Trainable params: 3865215 (14.74 MB)\n",
      "Non-trainable params: 2204 (8.61 KB)\n",
      "__________________________________________________________________________________________________\n",
      "[12-04-24 12:38:42 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:38:42 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[12-04-24 12:38:42 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:38:42 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 1: val_rmse improved from inf to 16.81718, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "/data/hpcdata/users/bryald/miniconda3/envs/icenet0.2.8/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "46/46 - 41s - loss: 5.8999 - binacc: 97.6127 - mae: 2.4343 - rmse: 5.6968 - mse: 1145.0914 - val_loss: 51.4159 - val_binacc: 95.1217 - val_mae: 9.8933 - val_rmse: 16.8172 - val_mse: 299.8471 - lr: 1.0000e-04 - 41s/epoch - 896ms/step\n",
      "[12-04-24 12:39:23 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 2: val_rmse improved from 16.81718 to 14.99649, saving model to ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "46/46 - 36s - loss: 5.3711 - binacc: 97.6962 - mae: 2.2758 - rmse: 5.4354 - mse: 1067.9620 - val_loss: 40.8856 - val_binacc: 95.6162 - val_mae: 8.8438 - val_rmse: 14.9965 - val_mse: 315.4923 - lr: 1.0000e-04 - 36s/epoch - 777ms/step\n",
      "[12-04-24 12:39:59 :INFO    ] - Saving network to: ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5\n",
      "[12-04-24 12:40:03 :INFO    ] - Running evaluation against test set\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:40:03 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:40:03 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:40:03 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:40:03 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[12-04-24 12:40:05 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:40:05 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[12-04-24 12:40:05 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:40:05 :INFO    ] - Using test set for validation\n",
      "[12-04-24 12:40:05 :INFO    ] - Metric creation for lead time of 7 days\n",
      "[12-04-24 12:40:05 :INFO    ] - Evaluating... \n",
      "[12-04-24 12:40:07 :INFO    ] - Done in 2.1s\n"
     ]
    }
   ],
   "source": [
    "!icenet_train tutorial_data tutorial_testrun 42 -b 2 -e 2 -m -qs 4 -w 4 -n 0.6 -nw \\\n",
    "    -p ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on training and prediction\n",
    "\n",
    "There are a few things to note about the `icenet_train` and `icenet_predict` (see [the prediction section below](#Predict)) commands and the switches they provide: \n",
    "\n",
    "* Common switches such as `-n` should be applied consistently between training and prediction. \n",
    "* These commands work with __individual network runs__ (see the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "## 5. Predict\n",
    "\n",
    "To run individual sets through the test network from the test dataset we produced earlier can be easily achieved. The steps are to create a date file, which can be produced from the configuration created by `icenet_process` in the [processing section](#Process). This date file then can be supplied to the `icenet_predict` command to produce files using either cached data (useful for test data prepared at the same time as the training and validation sets) or directly from the normalised data (as is the case for nearly all data that isn't part of the training run.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`icenet_predict` takes a file containing dates to make predictions for. First we can make a file, here called `testdates.csv` to pass to `icenet_predict` in the next step. (Note that the more advanced IceNet Pipeline method uses a more elegant system for providing dates; or if using the python interface, can be provided to the `predict_forecast` function as a list of dates - see 03.library_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01\n",
      "2020-04-02"
     ]
    }
   ],
   "source": [
    "!printf \"2020-04-01\\n2020-04-02\" | tee testdates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "usage: icenet_predict [-h] [-i IDENT] [-n N_FILTERS_FACTOR] [-l] [-t] [-v]\n",
      "                      [-s]\n",
      "                      dataset network_name output_name seed datefile\n",
      "\n",
      "positional arguments:\n",
      "  dataset\n",
      "  network_name\n",
      "  output_name\n",
      "  seed\n",
      "  datefile\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i IDENT, --train-identifier IDENT\n",
      "                        Train dataset identifier\n",
      "  -n N_FILTERS_FACTOR, --n-filters-factor N_FILTERS_FACTOR\n",
      "  -l, --legacy-rounding\n",
      "                        Ensure filter number rounding occurs last in channel\n",
      "                        number calculations\n",
      "  -t, --testset\n",
      "  -v, --verbose\n",
      "  -s, --save_args\n"
     ]
    }
   ],
   "source": [
    "!icenet_predict --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "[12-04-24 12:40:15 :INFO    ] - Loading configuration ./dataset_config.tutorial_data.json\n",
      "[12-04-24 12:40:15 :INFO    ] - Training dataset path: ./network_datasets/tutorial_data/south/train\n",
      "[12-04-24 12:40:15 :INFO    ] - Validation dataset path: ./network_datasets/tutorial_data/south/val\n",
      "[12-04-24 12:40:15 :INFO    ] - Test dataset path: ./network_datasets/tutorial_data/south/test\n",
      "[12-04-24 12:40:15 :INFO    ] - Loading configuration /data/hpcdata/users/bryald/git/icenet/icenet-notebooks/loader.tutorial_data.json\n",
      "[12-04-24 12:40:15 :INFO    ] - Loading model from ./results/networks/tutorial_testrun/tutorial_testrun.network_tutorial_data.42.h5...\n",
      "[12-04-24 12:40:15 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[12-04-24 12:40:16 :INFO    ] - Processing test batch 1, item 0 (date 2020-04-01)\n",
      "[12-04-24 12:40:16 :INFO    ] - Running prediction 2020-04-01\n",
      "[12-04-24 12:40:16 :INFO    ] - Saving 2020-04-01 - forecast output (1, 432, 432, 7)\n",
      "[12-04-24 12:40:16 :INFO    ] - Processing test batch 1, item 1 (date 2020-04-02)\n",
      "[12-04-24 12:40:16 :INFO    ] - Running prediction 2020-04-02\n",
      "[12-04-24 12:40:17 :WARNING ] - ./results/predict/example_south_forecast/tutorial_testrun.42 output already exists\n",
      "[12-04-24 12:40:17 :INFO    ] - Saving 2020-04-02 - forecast output (1, 432, 432, 7)\n"
     ]
    }
   ],
   "source": [
    "!icenet_predict -n 0.6 -t \\\n",
    "    tutorial_data tutorial_testrun example_south_forecast 42 testdates.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "The example uses the cached test data from the training run, but the process is the same for any other processed data with only the need to _omit the `-t` option, which specifies to source from cached test data_.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "In the above example, there are three outputs: \n",
    "\n",
    "* __forecast__: the ___predicted___ forecast data from the model output layer\n",
    "* __outputs__: the outputs from the data loader which would be used for training\n",
    "* __weights__: the generated sample weights from the data loader for the training sample\n",
    "\n",
    "The outputs initially are stored as Numpy arrays under the `results` directory thusly: \n",
    "\n",
    "```\n",
    "results/predict/example_south_forecast/notebook_testrun.42/2020_04_01.npy\n",
    "results/predict/example_south_forecast/notebook_testrun.42/2020_04_02.npy\n",
    "```\n",
    "\n",
    "With associated inputs, output and weights stored within subfolders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual numpy outputs, samples and sample weights are deposited into `/results/predict`. To generate __a CF-compliant NetCDF containing the forecasts requested__ we need to run `icenet_output`, these can then be post-processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "[12-04-24 12:40:20 :INFO    ] - Loading configuration ./dataset_config.tutorial_data.json\n",
      "[12-04-24 12:40:20 :INFO    ] - Training dataset path: ./network_datasets/tutorial_data/south/train\n",
      "[12-04-24 12:40:20 :INFO    ] - Validation dataset path: ./network_datasets/tutorial_data/south/val\n",
      "[12-04-24 12:40:20 :INFO    ] - Test dataset path: ./network_datasets/tutorial_data/south/test\n",
      "/data/hpcdata/users/bryald/miniconda3/envs/icenet0.2.8/lib/python3.11/site-packages/iris/__init__.py:354: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  cubes = _load_collection(uris, constraints, callback).cubes()\n",
      "[12-04-24 12:40:23 :INFO    ] - Post-processing 2020-04-01\n",
      "[12-04-24 12:40:23 :INFO    ] - Post-processing 2020-04-02\n",
      "[12-04-24 12:40:23 :INFO    ] - Dataset arr shape: (2, 432, 432, 7, 2)\n",
      "[12-04-24 12:40:23 :INFO    ] - Saving to results/predict/example_south_forecast.nc\n"
     ]
    }
   ],
   "source": [
    "!icenet_output example_south_forecast tutorial_data testdates.csv -o results/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the netCDF file containing the forecast, we can generate plots using `icenet_plot_forecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "usage: icenet_plot_forecast [-h] [-o OUTPUT_PATH] [-v] [-r REGION]\n",
      "                            [-l LEADTIMES] [-c] [-f {mp4,png,svg,tiff}]\n",
      "                            [-n CMAP_NAME] [-s]\n",
      "                            {north,south} forecast_file forecast_date\n",
      "\n",
      "positional arguments:\n",
      "  {north,south}\n",
      "  forecast_file\n",
      "  forecast_date\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -o OUTPUT_PATH, --output-path OUTPUT_PATH\n",
      "  -v, --verbose\n",
      "  -r REGION, --region REGION\n",
      "                        Region specified x1, y1, x2, y2\n",
      "  -l LEADTIMES, --leadtimes LEADTIMES\n",
      "                        Leadtimes to output, multiple as CSV, range as n..n\n",
      "  -c, --no-coastlines   Turn off cartopy integration\n",
      "  -f {mp4,png,svg,tiff}, --format {mp4,png,svg,tiff}\n",
      "                        Format to output in\n",
      "  -n CMAP_NAME, --cmap-name CMAP_NAME\n",
      "                        Color map name if not wanting to use default\n",
      "  -s, --stddev          Plot the standard deviation from the ensemble\n"
     ]
    }
   ],
   "source": [
    "!icenet_plot_forecast --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we generate video outputs for the two dates we've forecasted for over a 7 day period.\n",
    "\n",
    "`-l` defines the start and end lead times to capture in the video.\n",
    "\n",
    "`-o` defines the output directory for the image/video.\n",
    "\n",
    "`-f` defines the output file type.\n",
    "\n",
    "**Note:** For video outputs, this requires `ffmpeg` to be installed locally (Or, can remove the `-f mp4` flag to output a series of `png` forecast image files without `ffmpeg`).\n",
    "\n",
    "If using anaconda, you can install this using:\n",
    "\n",
    "```bash\n",
    "    conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not found - not required if not using PyTorch\n",
      "WARNING:root:No directory at: outputs\n",
      "INFO:root:Using cmap Blues_r\n",
      "WARNING:root:Coastlines will not work with the current implementation of xarray_to_video\n",
      "INFO:root:Inspecting data\n",
      "INFO:root:Initialising plot\n",
      "INFO:root:Animating\n",
      "INFO:root:Saving plot to outputs/example_south_forecast.2020-04-01.20200401.mp4\n",
      "PyTorch not found - not required if not using PyTorch\n",
      "INFO:root:Using cmap Blues_r\n",
      "WARNING:root:Coastlines will not work with the current implementation of xarray_to_video\n",
      "INFO:root:Inspecting data\n",
      "INFO:root:Initialising plot\n",
      "INFO:root:Animating\n",
      "INFO:root:Saving plot to outputs/example_south_forecast.2020-04-02.20200402.mp4\n"
     ]
    }
   ],
   "source": [
    "!icenet_plot_forecast south results/predict/example_south_forecast.nc 2020-04-01 -l 1..7 -o outputs -f mp4\n",
    "!icenet_plot_forecast south results/predict/example_south_forecast.nc 2020-04-02 -l 1..7 -o outputs -f mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more automated way of visualising the forecasts from the netCDF output is shown in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the video can be visualised for the two test dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/example_south_forecast.2020-04-01.20200401.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"outputs/example_south_forecast.2020-04-01.20200401.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/example_south_forecast.2020-04-02.20200402.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"outputs/example_south_forecast.2020-04-02.20200402.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "___\n",
    "## Summary\n",
    "\n",
    "Within this notebook we've attempted to give a full crash course to running the CLI tools __manually__. This is the first of a series of notebooks, covering further information: \n",
    "\n",
    "* [Data structure and analysis](03.data_and_forecasts.ipynb): understand the structure of the data stores and products created by these workflows and what tools currently exist in IceNet to looks over them.\n",
    "* [Library usage](04.library_usage.ipynb): understand how to programmatically perform an end to end run.\n",
    "* [Library extension](05.library_extension.ipynb): understand why and how to extend the IceNet library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- IceNet Codebase: v0.2.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
