{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in /data/hpcdata/users/jambyr/icenet/notebook-pipeline\n"
     ]
    }
   ],
   "source": [
    "# Quick hack to put us in the icenet-pipeline folder, assuming it was created as per 01.cli_demonstration.ipynb\n",
    "import os\n",
    "if os.path.exists(\"05.library_extension.ipynb\"):\n",
    "    os.chdir(\"../notebook-pipeline\")\n",
    "print(\"Running in {}\".format(os.getcwd()))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceNet Library Extension\n",
    "\n",
    "## Context\n",
    "\n",
    "### Purpose\n",
    "The IceNet library provides the ability to download, process, train and predict from end to end via a set of API/CLI tools.\n",
    "\n",
    "Using this notebook we will demonostrate how one can extend the functionality of the API provided by the library.\n",
    "\n",
    "### Highlights\n",
    "The key features of this notebook are:\n",
    "\n",
    "* [Reviewing the integration of MARS HRES data as an additional data source.](#Data:-Reviewing-MARS-HRES)\n",
    "* [Demonstrating the incorporation of a new datasource through notebook based development.](#Data:-Extending-with-another-implementation)\n",
    "* [Considerations when having extended the library.](#Considerations-when-extending)\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "James Byrne (author)\n",
    "\n",
    "__Please raise issues [in this repository](https://github.com/antarctica/IceNet-Pipeline) to suggest updates to this notebook!__ \n",
    "\n",
    "Contact me at _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    "\n",
    "#### Modelling codebase\n",
    "James Byrne (code author), Tom Andersson (science author)\n",
    "\n",
    "#### Modelling publications\n",
    "Andersson, T.R., Hosking, J.S., PÃ©rez-Ortiz, M. et al. Seasonal Arctic sea ice forecasting with probabilistic deep learning. Nat Commun 12, 5124 (2021). https://doi.org/10.1038/s41467-021-25257-4\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and British Antarctic Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data: Reviewing MARS HRES\n",
    "\n",
    "The first extension of the original codebase of IceNet, after having refactored from the original research, was in integrating the ECMWF HRES atmospheric data. This is [not readily available without agreeing to the license conditions](https://www.ecmwf.int/en/forecasts/datasets/catalogue-ecmwf-real-time-products), but for the sake of explanation it is possible to describe how to implement a new data stream.\n",
    "\n",
    "As mentioned in the [previous notebook](03.library_usage.ipynb), there are three types of data producers defined in the IceNet class hierarchy:\n",
    "\n",
    "* `Downloader` type producers; from which all `icenet.data.interfaces` implementations derive their functionality, as well as `SICDownloader` in `icenet.data.sic.osisaf`, which is our reference dataset for sea-ice concentration. Currently an additional class `ClimateDownloader` implements a load of extra atmospheric related functionality for the implementations in `icenet.data.interfaces` as well.\n",
    "* `Generator` type producers; which differ from `Downloader` in semantics only at present. Currently `IceNetDataLoader` and `Masks` use this as a base class. \n",
    "* `Processor` type producers; at present all concrete implementations descend from its child `IceNetPreProcessor` which defines common handling for variable naming, configuration generation, normalisation and linear trend forecasting. `Processor` itself also contains functionality that differs with `Downloader` and `Generator`, in that it implements source data store pickup functionality via `init_source_data`. \n",
    "\n",
    "The distinctions here are useful for the future and refactoring that is still ongoing to make common certain functionality that is duplicated. The overall class diagram looks as follows.\n",
    "\n",
    "<img src=\"classhierarchy.png\" alt=\"IceNet high level class diagram\" />\n",
    "\n",
    "The important thing to note is that all data producers derive from DataProducer which implements the key file handling routines that keep the data flow (*FIXME: at time of writing, relatively*) consistent. When implementing new data processing for use in an end to end pipeline, all that is needed will be: \n",
    "\n",
    "1. A concrete implementation of `Downloader` or `ClimateDownloader` that takes care of interfacing with the external system;\n",
    "1. A concrete implementation of `Processor` via `IceNetPreProcessor`, that defines any specific\n",
    "\n",
    "### Data Downloaders\n",
    "\n",
    "The first step to defining the new `ClimateDownloader` is to inherit from it and, in the case of MARS HRES data, define our request template and parameter map: this is not strongly defined but allows us to link the parameters to the equivalent channels used for training, as this data is used for predictions primarily at present.\n",
    "\n",
    "The most important thing to note is that data downloaded and used to train creates one or more channels in the input dataset. As such, if you say train with a *geopotential height from 500hpa* variable, you need to ensure that any other composite data source provides a channel whose named can be similarly derives. In this case, we had this channel called **zg500** in the ERA5 training data, so we supply this via **zg** (appended with 500 through initialisation of the class for this variable with a `[250, 500]` entry in `ClimateDownloader`s `pressure_levels` argument)...\n",
    "\n",
    "```python\n",
    "class HRESDownloader(ClimateDownloader):\n",
    "    PARAM_TABLE = 128\n",
    "    HRES_PARAMS = {\n",
    "        \"siconca\":      (31, \"siconc\"),     # sea_ice_area_fraction\n",
    "        \"tos\":          (34, \"sst\"),    # sea surface temperature \n",
    "        \"zg\":           (129, \"z\"),     # geopotential\n",
    "        \"ta\":           (130, \"t\"),     # air_temperature (t)\n",
    "        \"hus\":          (133, \"q\"),     # specific_humidity\n",
    "        \"psl\":          (134, \"sp\"),    # surface_pressure\n",
    "        \"uas\":          (165, \"u10\"),   # 10m_u_component_of_wind\n",
    "        \"vas\":          (166, \"v10\"),   # 10m_v_component_of_wind\n",
    "        \"tas\":          (167, \"t2m\"),   # 2m_temperature (t2m)\n",
    "        \"rlds\":         (175, \"strd\"),\n",
    "        \"rsds\":         (169, \"ssrd\"),\n",
    "    }\n",
    "\n",
    "    MARS_TEMPLATE = \"\"\"\n",
    "retrieve,\n",
    "  class=od,\n",
    "  date={date},\n",
    "  expver=1,\n",
    "  levtype={levtype},\n",
    "  {levlist}param={params},\n",
    "  step=0,\n",
    "  stream=oper,\n",
    "  time=00:00:00,\n",
    "  type=fc,\n",
    "  area={area},\n",
    "  grid=0.25/0.25,\n",
    "  target=\"{target}\",\n",
    "  format=netcdf\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Next, we define our constructor. Here the importance is in supplying an identifier which is used to identify the source data under the `/data/` directory. Notice it's also an opportunity to start up the API client for this particular service.\n",
    "\n",
    "The `HRESDownloader` is in itself interesting as down the line we actually instantiate **more than a single processor** for it, meaning that a single source data store under `/data/mars.hres/` is used to seed multiple preprocessing steps, as it provides `siconca` (our sea ice concentration channels) and atmospheric channels, which do and do not require linear trends generating and thus different configurations. \n",
    "\n",
    "```python\n",
    "    def __init__(self,\n",
    "                 *args,\n",
    "                 identifier=\"mars.hres\",\n",
    "                 **kwargs):\n",
    "        super().__init__(*args,\n",
    "                         identifier=identifier,\n",
    "                         **kwargs)\n",
    "\n",
    "        self._server = ecmwfapi.ECMWFService(\"mars\")\n",
    "```\n",
    "\n",
    "**The first of two abstract methods requiring implementation is `_get_dates_for_request`**, which is defined in `ClimateDownloader` at time of writing. THis returns a list of batches of dates, appropriate to the best practice for interacting with the external system, so that the request in `_single_download` can be called with each batch as the argument `req_dates`.\n",
    "\n",
    "```python\n",
    "    def _get_dates_for_request(self):\n",
    "        return batch_requested_dates(self._dates, attribute=\"month\")\n",
    "```\n",
    "\n",
    "**The second of the two abstract methods from `ClimateDownloader` requiring implementation is `_single_download`**. This takes every batch, the supplied requested variable names and associated pressures and prepares and issues the request, being responsible for downloading a file for the batch *and then splitting it into daily files in the source directory* ([see the data_and_forecasts notebook if that doesn't make sense to you!](02.data_and_forecasts.ipynb))\n",
    "\n",
    "[Please note this is subject to a future refactor as the data interfaces implementation has a bit of technical debt still to address to move common functionality back up the hierarchy](https://github.com/JimCircadian/icenet2/issues/9).\n",
    "\n",
    "Note in particular the importance of populating `self._files_downloaded` with newly downloaded files. Maintaining this list thus feeds the `regrid` and `rotate_wind_data` methods that are inherited from `ClimateDownloader` for use with each instantiation, if required.\n",
    "\n",
    "```python\n",
    "    def _single_download(self, var_names, pressures, req_dates):\n",
    "        levtype = \"plev\" if pressures else \"sfc\"\n",
    "\n",
    "        for dt in req_dates:\n",
    "            assert dt.year == req_dates[0].year\n",
    "            assert dt.month == req_dates[0].month\n",
    "\n",
    "        request_month = req_dates[0].strftime(\"%Y%m\")\n",
    "        request_target = \"{}.{}.{}.nc\".format(\n",
    "            self.hemisphere_str[0], levtype, request_month)\n",
    "\n",
    "        download_dates = []\n",
    "\n",
    "        for var_name, pressure in product(var_names, pressures.split('/')\n",
    "                                          if pressures else [None]):\n",
    "            var = var_name if not pressure else \\\n",
    "                \"{}{}\".format(var_name, pressure)\n",
    "            var_folder = os.path.join(self.get_data_var_folder(var),\n",
    "                                      str(req_dates[0].year))\n",
    "\n",
    "            for destination_date in req_dates:\n",
    "                daily_path, regridded_name = get_daily_filenames(\n",
    "                    var_folder, var, destination_date.strftime(\"%Y_%m_%d\"))\n",
    "\n",
    "                if not os.path.exists(daily_path) \\\n",
    "                        and not os.path.exists(regridded_name):\n",
    "                    if destination_date not in download_dates:\n",
    "                        download_dates.append(destination_date)\n",
    "                elif not os.path.exists(regridded_name):\n",
    "                    self._files_downloaded.append(daily_path)\n",
    "\n",
    "        download_dates = sorted(list(set(download_dates)))\n",
    "\n",
    "        if not len(download_dates):\n",
    "            logging.info(\"We have all the files we need from MARS API\")\n",
    "            return\n",
    "\n",
    "        request = HRESDownloader.MARS_TEMPLATE.format(\n",
    "            area=\"/\".join([str(s) for s in self.hemisphere_loc]),\n",
    "            date=\"/\".join([el.strftime(\"%Y%m%d\") for el in download_dates]),\n",
    "            levtype=levtype,\n",
    "            levlist=\"levelist={},\\n  \".format(pressures) if pressures else \"\",\n",
    "            params=\"/\".join(\n",
    "                [\"{}.{}\".format(\n",
    "                    HRESDownloader.HRES_PARAMS[v][0],\n",
    "                    HRESDownloader.PARAM_TABLE)\n",
    "                 for v in var_names]),\n",
    "            target=request_target,\n",
    "        )\n",
    "\n",
    "        logging.debug(\"MARS REQUEST: \\n{}\\n\".format(request))\n",
    "\n",
    "        if not os.path.exists(request_target):\n",
    "            self._server.execute(request, request_target)\n",
    "\n",
    "        ds = xr.open_dataset(request_target)\n",
    "\n",
    "        for day in ds.time.values:\n",
    "            date_str = pd.to_datetime(day).strftime(\"%Y_%m_%d\")\n",
    "\n",
    "            for var_name, pressure in product(var_names, pressures.split('/')\n",
    "                                              if pressures else [None]):\n",
    "                var = var_name if not pressure else \\\n",
    "                    \"{}{}\".format(var_name, pressure)\n",
    "                var_folder = os.path.join(self.get_data_var_folder(var),\n",
    "                                          str(pd.to_datetime(day).year))\n",
    "\n",
    "                # For the year component - 365 * 50 is a lot of files ;)\n",
    "                os.makedirs(var_folder, exist_ok=True)\n",
    "\n",
    "                daily_path, _ = get_daily_filenames(var_folder, var, date_str)\n",
    "\n",
    "                da = getattr(ds,\n",
    "                             HRESDownloader.HRES_PARAMS[var_name][1])\n",
    "\n",
    "                if pressure:\n",
    "                    da = da.sel(level=int(pressure))\n",
    "\n",
    "                # Just to make sure\n",
    "                da_daily = da.sel(time=slice(\n",
    "                    pd.to_datetime(day), pd.to_datetime(day)))\n",
    "\n",
    "                logging.info(\"Saving new daily file: {}\".format(daily_path))\n",
    "                da_daily.to_netcdf(daily_path)\n",
    "\n",
    "                if daily_path not in self._files_downloaded:\n",
    "                    self._files_downloaded.append(daily_path)\n",
    "\n",
    "        logging.info(\"Removing {}\".format(request_target))\n",
    "        ds.close()\n",
    "        os.unlink(request_target)\n",
    "```\n",
    "\n",
    "***Finally, because the MARS API benefits from single level and pressure level variables being grouped together*** we define a **custom** implementation for the `ClimateDownloader.download` method. The original actually issues a `_single_download` request for each variable-level-req_date, which is not favoured for the ECMWF API. \n",
    "\n",
    "```python\n",
    "    def download(self):\n",
    "        logging.info(\"Building request(s), downloading and daily averaging \"\n",
    "                     \"from {} API\".format(self.identifier.upper()))\n",
    "\n",
    "        sfc_vars = [var for idx, var in enumerate(self.var_names)\n",
    "                    if not self.pressure_levels[idx]]\n",
    "        plev_vars = [var for idx, var in enumerate(self.var_names)\n",
    "                     if self.pressure_levels[idx]]\n",
    "        pressures = \"/\".join([str(s) for s in sorted(set(\n",
    "            [p for ps in self.pressure_levels if ps for p in ps]))])\n",
    "\n",
    "        dates_per_request = self._get_dates_for_request()\n",
    "\n",
    "        for req_batch in dates_per_request:\n",
    "            self._single_download(sfc_vars, None, req_batch)\n",
    "            self._single_download(plev_vars, pressures, req_batch)\n",
    "\n",
    "        logging.info(\"{} daily files downloaded\".\n",
    "                     format(len(self._files_downloaded)))\n",
    "```\n",
    "\n",
    "### Data Processors\n",
    "\n",
    "\n",
    "```python\n",
    "class IceNetHRESPreProcessor(IceNetPreProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args,\n",
    "                         file_filters=[\"latlon_\"],\n",
    "                         identifier=\"mars.hres\",\n",
    "                         **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data: Extending with another implementation\n",
    "\n",
    "TODO\n",
    "\n",
    "## Other extensions\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Considerations when extending\n",
    "\n",
    "### Open a PR!\n",
    "\n",
    "The IceNet library is (will be, at present time) open sourced so that people can contribute back to it. Therefore, if you've implemented a useful downloader, processor or other item of functionality **the community will definitely benefit from it!** Please do contribute via a pull request, even if it's a quick and dirty implementation. \n",
    "\n",
    "### Documentation\n",
    "\n",
    "If possible, preparing even a small amount of documentation will go a long way, especially if it points people at external sites describing data or illustrates the reasoning/usage of the new functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- Codebase: drafting for v0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-icenet]",
   "language": "python",
   "name": "conda-env-miniconda3-icenet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
